{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Limiting cpu threads to: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the number of threads for all relevant libraries\n",
    "num_threads = \"8\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = num_threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = num_threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = num_threads\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = num_threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = num_threads\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    torch.set_num_threads(8)\n",
    "    print(f\"Limiting cpu threads to: {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh files...\n",
      "Combining and cleaning meshes...\n",
      "Mesh processing complete.\n",
      "Generating collocation points in batches...\n",
      "  Found 5099 / 5000 points...\n",
      "Generating fixed boundary points in batches...\n",
      "  Found 1074 / 1000 points...\n",
      "Generated 5000 collocation points.\n",
      "Generated 1000 fixed boundary points.\n",
      "\n",
      "Creating and saving simplified mesh for visualization...\n",
      "Simplified mesh saved to 'visualization_mesh.vtk'\n",
      "\n",
      "Point generation complete and visualization skipped.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =============================================================================\n",
    "# 1. POINT GENERATION FROM STL FILES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Parameters ---\n",
    "# FIX: Reduced the number of points to prevent memory overload.\n",
    "# Start with smaller numbers and increase them later if needed.\n",
    "n_collocation_points = 5000\n",
    "n_boundary_points = 1000\n",
    "\n",
    "# --- Load and Process Geometries ---\n",
    "try:\n",
    "    print(\"Loading mesh files...\")\n",
    "    coil_mesh = pv.read('coil.stl')\n",
    "    housing_mesh = pv.read('housing.stl') # Assuming this is the correct filename\n",
    "\n",
    "    # FIX: A more robust method to create a single, watertight surface\n",
    "    print(\"Combining and cleaning meshes...\")\n",
    "    # 1. Merge the meshes into a single volumetric representation.\n",
    "    combined_volume = pv.merge([coil_mesh, housing_mesh])\n",
    "    # 2. Extract the outer surface of the combined volume.\n",
    "    surface = combined_volume.extract_surface()\n",
    "    # 3. Clean the surface (merges duplicate points, etc.) and then fill holes.\n",
    "    combined_mesh = surface.clean().fill_holes(hole_size=200.0)\n",
    "    print(\"Mesh processing complete.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure your STL files are in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "## --- Generate Collocation Points ---\n",
    "#print(\"Generating collocation points...\")\n",
    "#combined_bounds = combined_mesh.bounds\n",
    "#random_points_np = np.random.uniform(\n",
    "#    low=[combined_bounds[0], combined_bounds[2], combined_bounds[4]],\n",
    "#    high=[combined_bounds[1], combined_bounds[3], combined_bounds[5]],\n",
    "#    size=(n_collocation_points * 2, 3)  # Oversample\n",
    "#)\n",
    "#\n",
    "## Correct usage of select_enclosed_points:\n",
    "## 1. Create a PolyData object from the random points.\n",
    "#point_cloud = pv.PolyData(random_points_np)\n",
    "## 2. Call select_enclosed_points ON the point_cloud, passing the mesh as the surface.\n",
    "#selection = point_cloud.select_enclosed_points(combined_mesh, tolerance=1e-6)\n",
    "## 3. FIX: Get the boolean mask from the result's point_data.\n",
    "#selected_mask = selection.point_data['SelectedPoints'].astype(bool)\n",
    "## 4. Use this mask to select points from the original numpy array.\n",
    "#collocation_points_np = random_points_np[selected_mask]\n",
    "\n",
    "\n",
    "print(\"Generating collocation points in batches...\")\n",
    "combined_bounds = combined_mesh.bounds\n",
    "collocation_points_list = []\n",
    "batch_size = 2000 # Process 2000 random points at a time\n",
    "\n",
    "# Keep generating batches until we have enough points\n",
    "while len(collocation_points_list) < n_collocation_points:\n",
    "    # Generate one batch of random points\n",
    "    random_points_np = np.random.uniform(\n",
    "        low=[combined_bounds[0], combined_bounds[2], combined_bounds[4]],\n",
    "        high=[combined_bounds[1], combined_bounds[3], combined_bounds[5]],\n",
    "        size=(batch_size, 3)\n",
    "    )\n",
    "\n",
    "    # This is the corrected way to call select_enclosed_points:\n",
    "    # 1. Create a PolyData object from the random points.\n",
    "    point_cloud = pv.PolyData(random_points_np)\n",
    "    # 2. Call the function ON the point_cloud, passing the mesh as the surface.\n",
    "    selection = point_cloud.select_enclosed_points(combined_mesh, tolerance=1e-6)\n",
    "    # 3. Get the boolean mask from the result.\n",
    "    selected_mask = selection.point_data['SelectedPoints'].astype(bool)\n",
    "    \n",
    "    # Add the points that were inside the mesh to our list\n",
    "    enclosed_points = random_points_np[selected_mask]\n",
    "    if len(enclosed_points) > 0:\n",
    "        collocation_points_list.extend(enclosed_points)\n",
    "    \n",
    "    # Optional: Print progress\n",
    "    print(f\"\\r  Found {len(collocation_points_list)} / {n_collocation_points} points...\", end=\"\")\n",
    "\n",
    "# Combine all found points from the batches into a single array\n",
    "collocation_points_np = np.array(collocation_points_list)\n",
    "# Trim to the exact number desired, in case we overshot\n",
    "if len(collocation_points_np) > n_collocation_points:\n",
    "    collocation_points_np = collocation_points_np[:n_collocation_points]\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "# --- Generate Boundary Points ---\n",
    "#print(\"Generating fixed boundary points...\")\n",
    "#all_surface_vertices = combined_mesh.points\n",
    "#num_to_sample = min(n_boundary_points * 5, len(all_surface_vertices))\n",
    "#random_indices = np.random.choice(len(all_surface_vertices), num_to_sample, replace=False)\n",
    "#surface_points_array = all_surface_vertices[random_indices]\n",
    "#\n",
    "#bottom_z = combined_bounds[4]\n",
    "#fixed_boundary_points_np = surface_points_array[\n",
    "#    np.isclose(surface_points_array[:, 2], bottom_z)\n",
    "#]\n",
    "\n",
    "print(\"\\nGenerating fixed boundary points in batches...\")\n",
    "bottom_z = combined_bounds[4]\n",
    "fixed_boundary_points_list = []\n",
    "total_surface_vertices = combined_mesh.points\n",
    "\n",
    "# Keep sampling until we find enough points on the bottom surface\n",
    "while len(fixed_boundary_points_list) < n_boundary_points:\n",
    "    # Take a random sample of the mesh's existing vertices\n",
    "    random_indices = np.random.choice(len(total_surface_vertices), batch_size, replace=False)\n",
    "    surface_points_batch = total_surface_vertices[random_indices]\n",
    "    \n",
    "    # Filter this smaller batch for points on the bottom\n",
    "    bottom_points_batch = surface_points_batch[\n",
    "        np.isclose(surface_points_batch[:, 2], bottom_z)\n",
    "    ]\n",
    "\n",
    "    # Add any found points to our list\n",
    "    if len(bottom_points_batch) > 0:\n",
    "        fixed_boundary_points_list.extend(bottom_points_batch)\n",
    "\n",
    "    print(f\"\\r  Found {len(fixed_boundary_points_list)} / {n_boundary_points} points...\", end=\"\")\n",
    "\n",
    "\n",
    "# Combine all found points and trim to the exact number\n",
    "fixed_boundary_points_np = np.array(fixed_boundary_points_list)\n",
    "if len(fixed_boundary_points_np) > n_boundary_points:\n",
    "    fixed_boundary_points_np = fixed_boundary_points_np[:n_boundary_points]\n",
    "\n",
    "#==========================================================================================\n",
    "\n",
    "print(f\"\\nGenerated {len(collocation_points_np)} collocation points.\")\n",
    "print(f\"Generated {len(fixed_boundary_points_np)} fixed boundary points.\")\n",
    "\n",
    "# --- Convert to Tensors for PINN ---\n",
    "collocation_points = torch.tensor(collocation_points_np, dtype=torch.float32, requires_grad=True)\n",
    "fixed_boundary_points = torch.tensor(fixed_boundary_points_np, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#==================making a simpler vtk file for use in visualising the result of the PINN==================\n",
    "\n",
    "print(\"\\nCreating and saving simplified mesh for visualization...\")\n",
    "vis_mesh = combined_mesh.decimate(target_reduction=0.0)\n",
    "vis_mesh_filename = 'visualization_mesh.vtk'\n",
    "vis_mesh.save(vis_mesh_filename)\n",
    "print(f\"Simplified mesh saved to '{vis_mesh_filename}'\")\n",
    "\n",
    "del combined_mesh\n",
    "\n",
    "# --- Visualization (to verify the result) ---\n",
    "#print(\"\\nDisplaying generated points...\")\n",
    "#plotter = pv.Plotter()\n",
    "#plotter.add_mesh(combined_mesh, style='wireframe', color='gray', opacity=0.1)\n",
    "#plotter.add_points(collocation_points_np, color='blue', point_size=2, render_points_as_spheres=True, label='Collocation Points')\n",
    "#plotter.add_points(fixed_boundary_points_np, color='red', point_size=4, render_points_as_spheres=True, label='Fixed Boundary Points')\n",
    "#plotter.add_legend()\n",
    "#plotter.show()\n",
    "\n",
    "print(\"\\nPoint generation complete and visualization skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DEFINE MATERIAL AND FORCE PROPERTIES FOR THE NEW PROBLEM\n",
    "# =============================================================================\n",
    "\n",
    "# Define a Single Material (e.g., Aluminum)\n",
    "E_MATERIAL = 69e9  # Pascals\n",
    "NU_MATERIAL = 0.33\n",
    "\n",
    "# Define Forces\n",
    "# NOTE: The coordinates are based on your CadQuery script (in mm).\n",
    "# The top surface of the top plate is at z = coil_height + plate_thickness = 85 + 5 = 90.\n",
    "x_forces = torch.tensor([\n",
    "    [150.0, 0.0, 90.0], # Force on top plate\n",
    "    [-150.0, 0.0, 90.0] # Force on top plate\n",
    "])\n",
    "P_forces = torch.tensor([\n",
    "    [0.0, 0.0, -500.0], # 500N downward force\n",
    "    [0.0, 0.0, -500.0]  # 500N downward force\n",
    "])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PINN IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "# Your Neural Network class\n",
    "# NOTE: The name is changed to reflect the new problem, but the structure is identical.\n",
    "class PINN_Helmholtz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: The hard-coded boundary condition `(x**2) * nn_output` from the\n",
    "        # cantilever beam is removed, as it does not apply to this geometry.\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "\n",
    "def get_gradient(y, x):\n",
    "    \"\"\"Computes the gradient of y (N, 3) with respect to x (N, 3).\"\"\"\n",
    "    grad_u = torch.autograd.grad(y[:, 0], x, grad_outputs=torch.ones_like(y[:, 0]), create_graph=True)[0]\n",
    "    grad_v = torch.autograd.grad(y[:, 1], x, grad_outputs=torch.ones_like(y[:, 1]), create_graph=True)[0]\n",
    "    grad_w = torch.autograd.grad(y[:, 2], x, grad_outputs=torch.ones_like(y[:, 2]), create_graph=True)[0]\n",
    "    return torch.stack([grad_u, grad_v, grad_w], dim=1)\n",
    "\n",
    "\n",
    "# Your loss_fn_strainenergy, slightly adapted\n",
    "# NOTE: The function signature is simplified as it only needs the interior points.\n",
    "def loss_fn_strainenergy(model, x_interior, E, nu):\n",
    "    u_interior = model(x_interior)\n",
    "    grad_u = get_gradient(u_interior, x_interior)\n",
    "\n",
    "    epsilon_xx = grad_u[:, 0, 0]\n",
    "    epsilon_yy = grad_u[:, 1, 1]\n",
    "    epsilon_zz = grad_u[:, 2, 2]\n",
    "    epsilon_xy = 0.5 * (grad_u[:, 0, 1] + grad_u[:, 1, 0])\n",
    "    epsilon_xz = 0.5 * (grad_u[:, 0, 2] + grad_u[:, 2, 0])\n",
    "    epsilon_yz = 0.5 * (grad_u[:, 1, 2] + grad_u[:, 2, 1])\n",
    "\n",
    "    lmbda = (E * nu) / ((1 + nu) * (1 - 2 * nu))\n",
    "    mu = E / (2 * (1 + nu))\n",
    "    tr_epsilon = epsilon_xx + epsilon_yy + epsilon_zz\n",
    "\n",
    "    sigma_xx = lmbda * tr_epsilon + 2 * mu * epsilon_xx\n",
    "    sigma_yy = lmbda * tr_epsilon + 2 * mu * epsilon_yy\n",
    "    sigma_zz = lmbda * tr_epsilon + 2 * mu * epsilon_zz\n",
    "    sigma_xy = 2 * mu * epsilon_xy\n",
    "    sigma_xz = 2 * mu * epsilon_xz\n",
    "    sigma_yz = 2 * mu * epsilon_yz\n",
    "\n",
    "    strain_energy_density = 0.5 * (\n",
    "        sigma_xx * epsilon_xx + sigma_yy * epsilon_yy + sigma_zz * epsilon_zz +\n",
    "        2 * (sigma_xy * epsilon_xy + sigma_xz * epsilon_xz + sigma_yz * epsilon_yz)\n",
    "    )\n",
    "    \n",
    "    # NOTE: The domain_volume multiplication is removed. For complex shapes,\n",
    "    # simply taking the mean of the density is standard practice for the loss.\n",
    "    strain_energy = torch.mean(strain_energy_density)\n",
    "    return strain_energy\n",
    "\n",
    "\n",
    "# Your exact loss_fn_work\n",
    "def loss_fn_work(model, x_forces, P_forces):\n",
    "    u_at_forces = model(x_forces)\n",
    "    work_potential = torch.sum(P_forces * u_at_forces)\n",
    "    return work_potential\n",
    "\n",
    "\n",
    "# Your loss_fn_bc, simplified for the new problem\n",
    "# NOTE: The function signature is simplified as it only needs the boundary points.\n",
    "def loss_fn_bc(model, x_boundary):\n",
    "    u_boundary = model(x_boundary)\n",
    "    loss_displacement = torch.mean(u_boundary.pow(2))\n",
    "\n",
    "    # NOTE: The slope loss term is removed. For this problem, we are only\n",
    "    # enforcing that the displacement is zero, not that the slope is zero.\n",
    "    # grad_u_boundary = get_gradient(u_boundary, x_boundary)\n",
    "    # loss_slope = torch.mean(grad_u_boundary.pow(2))\n",
    "    # loss_bc = loss_displacement + loss_slope\n",
    "    \n",
    "    return loss_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR A VON MISES STRESS HEATMAP\n",
    "\n",
    "def stress_visualisation(model, original_mesh):\n",
    "    print(\"\\n--- Starting Stress Visualization (Memory-Efficient) ---\")\n",
    "    \n",
    "    # --- 1. Get all evaluation points from the original mesh ---\n",
    "    #eval_points_np = original_mesh.points\n",
    "\n",
    "    try:\n",
    "        vis_mesh = pv.read('visualization_mesh.vtk')\n",
    "        print(f\"Loaded simplified mesh with {vis_mesh.n_points} points.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'visualization_mesh.vtk' not found. Please run the point generation script first.\")\n",
    "        return\n",
    "    \n",
    "    #print(f\"Original mesh has {original_mesh.n_points} points.\")\n",
    "    #print(\"Simplifying mesh for visualization...\")\n",
    "    #vis_mesh = original_mesh.decimate(target_reduction=0.95)\n",
    "    #print(f\"Simplified mesh has {vis_mesh.n_points} points.\")\n",
    "\n",
    "    # --- 1. Get all evaluation points from the new, simplified mesh ---\n",
    "    eval_points_np = vis_mesh.points\n",
    "    \n",
    "    # --- 2. Process points in batches to calculate stress ---\n",
    "    print(\"Calculating stress field in batches...\")\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    \n",
    "    batch_size = 500  # You can adjust this size based on your available RAM\n",
    "    von_mises_stress_list = []\n",
    "\n",
    "    for i in range(0, len(eval_points_np), batch_size):\n",
    "        # Get a small chunk of points\n",
    "        points_batch_np = eval_points_np[i : i + batch_size]\n",
    "        points_batch = torch.tensor(points_batch_np, dtype=torch.float32, requires_grad=True).to(next(model.parameters()).device)\n",
    "        \n",
    "        # --- Perform calculations only on the small batch ---\n",
    "        u_pred = model(points_batch)\n",
    "        grad_u = get_gradient(u_pred, points_batch)\n",
    "        \n",
    "        E = E_MATERIAL\n",
    "        nu = NU_MATERIAL\n",
    "        lmbda = (E * nu) / ((1 + nu) * (1 - 2 * nu))\n",
    "        mu = E / (2 * (1 + nu))\n",
    "\n",
    "        epsilon_xx = grad_u[:, 0, 0]\n",
    "        epsilon_yy = grad_u[:, 1, 1]\n",
    "        epsilon_zz = grad_u[:, 2, 2]\n",
    "        epsilon_xy = 0.5 * (grad_u[:, 0, 1] + grad_u[:, 1, 0])\n",
    "        epsilon_xz = 0.5 * (grad_u[:, 0, 2] + grad_u[:, 2, 0])\n",
    "        epsilon_yz = 0.5 * (grad_u[:, 1, 2] + grad_u[:, 2, 1])\n",
    "        tr_epsilon = epsilon_xx + epsilon_yy + epsilon_zz\n",
    "\n",
    "        sigma_xx = lmbda * tr_epsilon + 2 * mu * epsilon_xx\n",
    "        sigma_yy = lmbda * tr_epsilon + 2 * mu * epsilon_yy\n",
    "        sigma_zz = lmbda * tr_epsilon + 2 * mu * epsilon_zz\n",
    "        sigma_xy = 2 * mu * epsilon_xy\n",
    "        sigma_xz = 2 * mu * epsilon_xz\n",
    "        sigma_yz = 2 * mu * epsilon_yz\n",
    "\n",
    "        term1 = (sigma_xx - sigma_yy)**2 + (sigma_yy - sigma_zz)**2 + (sigma_zz - sigma_xx)**2\n",
    "        term2 = 6 * (sigma_xy**2 + sigma_yz**2 + sigma_xz**2)\n",
    "        von_mises_stress_batch = torch.sqrt(0.5 * (term1 + term2)).cpu().detach().numpy()\n",
    "        \n",
    "        # Add the results for this batch to our list\n",
    "        von_mises_stress_list.append(von_mises_stress_batch)\n",
    "        \n",
    "        print(f\"\\r  Processed {i + len(points_batch_np)} / {len(eval_points_np)} points...\", end=\"\")\n",
    "\n",
    "    # Combine the results from all batches into a single array\n",
    "    von_mises_stress = np.concatenate(von_mises_stress_list)\n",
    "    print(\"\\nStress calculation complete.\")\n",
    "\n",
    "    # --- 3. Add Stress Data to the Mesh and Plot ---\n",
    "    mesh_with_stress = original_mesh.copy()\n",
    "    mesh_with_stress['Von Mises Stress'] = von_mises_stress\n",
    "\n",
    "    print(\"Displaying stress plot...\")\n",
    "    plotter_stress = pv.Plotter()\n",
    "    plotter_stress.add_mesh(mesh_with_stress, scalars='Von Mises Stress', cmap='plasma')\n",
    "    plotter_stress.camera_position = 'iso'\n",
    "    plotter_stress.add_text('Von Mises Stress', font_size=15)\n",
    "    plotter_stress.show()\n",
    "\n",
    "    # --- 4. Save the Resulting Mesh to a VTK File ---\n",
    "    output_filename = 'helmholtz_stress_result.vtk'\n",
    "    mesh_with_stress.save(output_filename)\n",
    "    print(f\"\\nStress results saved to {output_filename}. You can open this file in Paraview or other VTK viewers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement_magnitude_plot(model, original_mesh):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the DISPLACEMENT on the geometry and saves\n",
    "    the result to a VTK file. This version is simplified to avoid memory\n",
    "    issues from stress calculation.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained PINN model.\n",
    "        original_mesh: The PyVista mesh object of the geometry.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Simplified Visualization (Displacement Only) ---\")\n",
    "    \n",
    "    # --- 1. Simplify the mesh for visualization ---\n",
    "    print(f\"Original mesh has {original_mesh.n_points} points.\")\n",
    "    print(\"Simplifying mesh for visualization...\")\n",
    "    vis_mesh = original_mesh.decimate(target_reduction=0.95)\n",
    "    print(f\"Simplified mesh has {vis_mesh.n_points} points.\")\n",
    "    \n",
    "    # --- 2. Get evaluation points from the simplified mesh ---\n",
    "    eval_points_np = vis_mesh.points\n",
    "    eval_points = torch.tensor(eval_points_np, dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    \n",
    "    # --- 3. Predict Displacements (no gradients needed) ---\n",
    "    print(\"Predicting displacements...\")\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        displacements_pred = model(eval_points).cpu().numpy()\n",
    "\n",
    "    # Create the deformed mesh by adding displacements to original points\n",
    "    # A scaling factor can be used to exaggerate the deformation\n",
    "    deformation_scale_factor = 50 \n",
    "    deformed_points_np = eval_points_np + displacements_pred * deformation_scale_factor\n",
    "    \n",
    "    # Create a new PyVista mesh object with the deformed geometry\n",
    "    deformed_mesh = pv.PolyData(deformed_points_np, faces=vis_mesh.faces)\n",
    "\n",
    "    # Calculate the magnitude of the displacement vector for the heatmap\n",
    "    displacement_magnitude = np.linalg.norm(displacements_pred, axis=1)\n",
    "    deformed_mesh['Displacement Magnitude'] = displacement_magnitude\n",
    "\n",
    "    # --- 4. Plot the Displacement ---\n",
    "    #print(\"Displaying displacement plot...\")\n",
    "    #plotter_disp = pv.Plotter()\n",
    "    #plotter_disp.add_mesh(deformed_mesh, scalars='Displacement Magnitude', cmap='viridis')\n",
    "    #plotter_disp.camera_position = 'iso'\n",
    "    #plotter_disp.add_text('Displacement Magnitude', font_size=15)\n",
    "    #plotter_disp.show()\n",
    "\n",
    "    # --- 5. Save the Resulting Mesh to a VTK File ---\n",
    "    output_filename = 'helmholtz_displacement_result.vtk'\n",
    "    deformed_mesh.save(output_filename)\n",
    "    print(f\"\\nDisplacement results saved to {output_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. YOUR TRAINING LOOP (ADAPTED FOR THE NEW PROBLEM)\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize Model and Optimizer\n",
    "model = PINN_Helmholtz()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # NOTE: A smaller learning rate is often more stable\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(5001): # NOTE: Reduced epochs for initial testing\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate each component of the loss using your functions\n",
    "    # NOTE: The arguments passed to the functions are updated for the new variables\n",
    "    loss_U = loss_fn_strainenergy(model, collocation_points, E_MATERIAL, NU_MATERIAL)\n",
    "    loss_W = loss_fn_work(model, x_forces, P_forces)\n",
    "    loss_BC = loss_fn_bc(model, fixed_boundary_points)\n",
    "    \n",
    "    # Combine the losses using the principle of minimum potential energy (U - W)\n",
    "    # NOTE: The combination is U - W + BC_penalty. Your original code had U + W,\n",
    "    # which is physically incorrect. This version follows the energy principle.\n",
    "    total_loss = loss_U - loss_W + 1e6 * loss_BC\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {total_loss.item():.6e}\")\n",
    "    \n",
    "stress_visualisation(model, vis_mesh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
