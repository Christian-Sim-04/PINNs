{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from case1_datagen import generate_collocation_points, generate_interface_collocation_points\n",
    "from case1_loss_fns import pde_loss, bc_loss, interface_loss, flexural_rigidity, normalise, denormalise\n",
    "from case1_beamdoublenet import BeamDoubleNet\n",
    "from case1_bayesian_opt import run_bayesian_optimisation\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    torch.set_num_threads(16)\n",
    "    print(f\"Limiting cpu threads to: {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume the following functions are defined elsewhere:\n",
    "# def generate_collocation_points(n, x_ranges): ...\n",
    "# def normalise(x, xmin, xmax): ...\n",
    "# def generate_interface_collocation_points(n, n_interface, x_ranges, interface_x, interface_width): ...\n",
    "\n",
    "\n",
    "# --- Your Data Generation Parameters ---\n",
    "n_collocation = 3000\n",
    "#n_interface = 1000\n",
    "#x_ranges = [(0,2), (2,3)]\n",
    "#interface_x = 2.0\n",
    "#interface_width = 0.1\n",
    "xmin, xmax = 0.0, 3.0\n",
    "q0 = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Generate Uniform Collocation Points for Each Domain\n",
    "\n",
    "x_phys_pde = np.random.uniform(xmin, xmax, (n_collocation, 1)).astype(np.float32)\n",
    "\n",
    "# --- Convert to PyTorch Tensor and Finalize ---\n",
    "# Normalize the points\n",
    "x_pde_torch = torch.from_numpy(normalise(x_phys_pde, xmin, xmax)).to(torch.float32).to(device)\n",
    "\n",
    "shuffled_indices = torch.randperm(len(x_pde_torch))\n",
    "x_pde_torch = x_pde_torch[shuffled_indices]\n",
    "x_pde_torch.requires_grad_(True)\n",
    "\n",
    "data = {\n",
    "    'x_pde': x_pde_torch,  # Single tensor for all PDE points\n",
    "    'xmin': xmin,\n",
    "    'xmax': xmax,\n",
    "    'device': device,\n",
    "    'q0': q0,\n",
    "}\n",
    "\n",
    "print(f\"Total PDE points for the single beam: {len(x_pde_torch)}\")\n",
    "\n",
    "\n",
    "# --- VISUALISATION (Simplified) ---\n",
    "x_pde_combined_flat = x_pde_torch.cpu().detach().numpy().flatten() # Get flat array for plotting\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.scatter(x_pde_combined_flat, np.zeros_like(x_pde_combined_flat), alpha=0.5, label='Single Beam PDE points (normalized)')\n",
    "# No interface line needed for a single beam\n",
    "plt.xlabel('Normalized x')\n",
    "plt.yticks([])\n",
    "plt.title('Collocation Points Along Single Beam')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian search for the optimal parameters\n",
    "\n",
    "#epochs_per_trial = 300  # A small number of epochs for the search\n",
    "#n_trials = 50           # A reasonable number of trials\n",
    "\n",
    "#print(\"\\n--- Starting Bayesian Optimization Search ---\")\n",
    "#best_params = run_bayesian_optimisation(data, epochs_per_trial, n_trials, device)\n",
    "#print(\"\\n--- Search Complete. Best Parameters Found ---\")\n",
    "#print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "\n",
    "n_units=40\n",
    "n_layers=4\n",
    "pde_weight = 1.0#best_params['pde_weight']\n",
    "bc_weight = 1.0#best_params['bc_weight']\n",
    "#if_weight = best_params['if_weight']\n",
    "#if_cont_weight = best_params['if_cont_weight']\n",
    "#if_shear_weight = best_params['if_shear_weight']\n",
    "lr = 0.01#best_params['learning_rate']\n",
    "\n",
    "\n",
    "model_single_beam = BeamDoubleNet(\n",
    "    input_dim=1, output_dim=2,\n",
    "    n_units=n_units, n_layers=n_layers,\n",
    "    pde_weight=pde_weight, bc_weight=bc_weight, if_weight=if_weight,\n",
    "    #if_cont_weight=if_cont_weight, if_shear_weight=if_shear_weight,\n",
    ").to(device)\n",
    "\n",
    "#model_beam2 = BeamDoubleNet(\n",
    "#    input_dim=1, output_dim=2,\n",
    "#    n_units=n_units, n_layers=n_layers,\n",
    "#    pde_weight=pde_weight, bc_weight=bc_weight,# if_weight=if_weight,\n",
    "#    #if_cont_weight=if_cont_weight, if_shear_weight=if_shear_weight,\n",
    "#).to(device)\n",
    "\n",
    "\n",
    "# Set up the optimizer (a single optimizer to update the params of BOTH models)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model_single_beam.parameters()),\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "losses = []\n",
    "epochs=500\n",
    "\n",
    "for ep in range(epochs):\n",
    "    # This ensures x_pde_torch has requires_grad=True, even in a notebook\n",
    "    #x1_pde_torch.requires_grad_(True)\n",
    "    #x2_pde_torch.requires_grad_(True)          REQUIRED WHEN MINIBATCHING\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_residual = pde_loss(model_single_beam, x_pde_torch, xmin, xmax, q0)# + pde_loss(model_beam2, x2_pde_torch, xmin, xmax)\n",
    "    loss_boundary = bc_loss(model_single_beam, xmin, xmax)\n",
    "    loss_interface = interface_loss(model_single_beam xmin, xmax), q0# if_shear_weight, if_cont_weight)\n",
    "\n",
    "    total_loss = pde_weight*loss_residual + bc_weight*loss_boundary #+ if_weight * loss_interface\n",
    "\n",
    "    #backpropagation and optimisation via pytorch\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(total_loss.item())\n",
    "\n",
    "    if ep % int(epochs/10) == 0:\n",
    "        print(f\"Epoch {ep}: Total Loss {total_loss.item():.4e} | \"\n",
    "              f\"PDE {loss_residual.item():.4e} | \"\n",
    "              f\"BC {loss_boundary.item():.4e}\")\n",
    "              #f\"IF {loss_interface.item():.4e}\")\n",
    "        \n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training Loss History (Domain Decomposition)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 1. Beam and material properties\n",
    "E, D = 210e9, 0.05\n",
    "I = np.pi/64 * D**4\n",
    "EI = E * I\n",
    "\n",
    "def w_analytic(x_phys, q0_val, EI_val, L_val):\n",
    "    \"\"\"\n",
    "    Calculates the analytic deflection for a uniformly loaded cantilever beam.\n",
    "    x_phys: physical x-coordinates (m)\n",
    "    q0_val: uniformly distributed load (N/m)\n",
    "    EI_val: flexural rigidity (Nm^2)\n",
    "    L_val: total beam length (m)\n",
    "    \"\"\"\n",
    "    w = (q0_val / EI_val) * (\n",
    "        -x_phys**4 / 24.0 + L_val * x_phys**3 / 6.0 - (L_val**2) * x_phys**2 / 4.0\n",
    "    )\n",
    "    return w\n",
    "\n",
    "# Generate x values and normalize\n",
    "xmin, xmax = 0.0, 3.0\n",
    "x_phys_plot = np.linspace(xmin, xmax, 500)\n",
    "\n",
    "# Evaluate analytic solution\n",
    "w_true = w_analytic(x_phys_plot, q0_val=500, EI_val=EI, L_val=3)\n",
    "\n",
    "x_norm_plot_torch = torch.from_numpy(normalise(x_phys_plot, xmin, xmax)).reshape(-1, 1).to(torch.float32).to(device)\n",
    "\n",
    "# Use boolean masks to apply the correct model to each segment\n",
    "\n",
    "\n",
    "\n",
    "w_pinn = np.zeros_like(x_phys_plot)\n",
    "\n",
    "# Set models to evaluation mode and get predictions\n",
    "model_single_beam.eval()\n",
    "with torch.no_grad():\n",
    "    w_pinn = model_single_beam(x_norm_plot_torch).cpu().numpy()[:, 0].flatten()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_phys_plot, w_analytic(x_phys_plot), 'r-', label='Analytic Solution')\n",
    "plt.plot(x_phys_plot, w_pinn, 'b--', label='PINN Prediction')\n",
    "plt.xlabel('x (m)')\n",
    "plt.ylabel('Deflection $w(x)$ (m)')\n",
    "plt.title('PINN vs Analytic Solution (Domain Decomposition)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_at_2m = 2.0\n",
    "x_at_3m = 3.0\n",
    "w_at_2m = w_analytic(np.array([x_at_2m]))[0] # [0] to get the scalar value from the array\n",
    "w_at_3m = w_analytic(np.array([x_at_3m]))[0]\n",
    "\n",
    "print(f\"w_at_2m: {w_at_2m}, w_at_3m: {w_at_3m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
